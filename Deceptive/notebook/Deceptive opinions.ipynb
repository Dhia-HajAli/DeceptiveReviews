{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer    \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import pickle\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('deceptive-opinion.csv')\n",
    "X=dataset[\"text\"]\n",
    "y = np.where(dataset['deceptive']=='truthful', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text\n",
    "def cleaning(X):\n",
    "    corpus=[]\n",
    "    for i in range(0,len(X)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',X[i])\n",
    "        review=review.lower()\n",
    "        review=review.split()\n",
    "        ps=PorterStemmer()\n",
    "        review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "        review=' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cleaning(X)\n",
    "cv=TfidfVectorizer()\n",
    "cv.fit(corpus)\n",
    "x=cv.transform(corpus).toarray()\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv, open(\"vectorizer.h5\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,criterion='entropy')\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = svm.SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhia\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier().fit(X_train, y_train)\n",
    "lg=LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.634375\n",
      "Precision score: 0.6319444444444444\n",
      "Recall score: 0.5870967741935483\n",
      "F1 score: 0.608695652173913\n",
      "\n",
      "Accuracy score: 0.871875\n",
      "Precision score: 0.8518518518518519\n",
      "Recall score: 0.8903225806451613\n",
      "F1 score: 0.8706624605678234\n",
      "\n",
      "Accuracy score: 0.875\n",
      "Precision score: 0.8807947019867549\n",
      "Recall score: 0.8580645161290322\n",
      "F1 score: 0.869281045751634\n",
      "\n",
      "Accuracy score: 0.875\n",
      "Precision score: 0.8807947019867549\n",
      "Recall score: 0.8580645161290322\n",
      "F1 score: 0.869281045751634\n",
      "\n",
      "Accuracy score: 0.9\n",
      "Precision score: 0.9019607843137255\n",
      "Recall score: 0.8903225806451613\n",
      "F1 score: 0.8961038961038961\n",
      "\n",
      "Accuracy score: 0.721875\n",
      "Precision score: 0.775\n",
      "Recall score: 0.6\n",
      "F1 score: 0.6763636363636363\n"
     ]
    }
   ],
   "source": [
    "#predition\n",
    "pred_nb=nb.predict(X_test)\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, pred_nb)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred_nb)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred_nb)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred_nb)))\n",
    "\n",
    "pred_rf=rf.predict(X_test)\n",
    "print('\\nAccuracy score: {}'.format(accuracy_score(y_test, pred_rf)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred_rf)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred_rf)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred_rf)))\n",
    "\n",
    "pred_svm= svm.predict(X_test)\n",
    "print('\\nAccuracy score: {}'.format(accuracy_score(y_test, pred_svm)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred_svm)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred_svm)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred_svm)))\n",
    "\n",
    "\n",
    "pred_lg= lg.predict(X_test)\n",
    "print('\\nAccuracy score: {}'.format(accuracy_score(y_test, pred_lg)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred_lg)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred_lg)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred_lg)))\n",
    "\n",
    "\n",
    "pred_knn= knn.predict(X_test)\n",
    "print('\\nAccuracy score: {}'.format(accuracy_score(y_test, pred_knn)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred_knn)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred_knn)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, pred_knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0]\n",
      "[1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1]\n",
      "[1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      "[1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      "[1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      "[0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      "\n",
      "[1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(nb.predict(X_test[0:30]))\n",
    "print(rf.predict(X_test[0:30]))\n",
    "print(svm.predict(X_test[0:30]))\n",
    "print(lg.predict(X_test[0:30]))\n",
    "print(knn.predict(X_test[0:30]))\n",
    "\n",
    "print(\"\\n\"+str(y_test[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[0.59 0.41]\n",
      " [0.3  0.7 ]\n",
      " [0.72 0.28]]\n",
      "[[0.8929425  0.1070575 ]\n",
      " [0.01256114 0.98743886]\n",
      " [0.97183386 0.02816614]]\n",
      "[[0.61030914 0.38969086]\n",
      " [0.23485585 0.76514415]\n",
      " [0.7481794  0.2518206 ]]\n",
      "[[0.6 0.4]\n",
      " [0.8 0.2]\n",
      " [0.8 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(nb.predict_proba(X_test[5:8]))\n",
    "print(rf.predict_proba(X_test[5:8]))\n",
    "print(svm.predict_proba(X_test[5:8]))\n",
    "print(lg.predict_proba(X_test[5:8]))\n",
    "print(knn.predict_proba(X_test[5:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = 'rf.h5'\n",
    "svm_model = 'svm.h5'\n",
    "lg_model = 'lg.h5'\n",
    "knn_model = 'knn.h5'\n",
    "pickle.dump(rf, open(rf_model, 'wb'))\n",
    "pickle.dump(svm, open(svm_model, 'wb'))\n",
    "pickle.dump(lg, open(lg_model, 'wb'))\n",
    "pickle.dump(knn, open(knn_model, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rf = pickle.load(open(rf_model, 'rb'))\n",
    "load_svm = pickle.load(open(svm_model, 'rb'))\n",
    "load_lg = pickle.load(open(lg_model, 'rb'))\n",
    "load_knn = pickle.load(open(knn_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing(X):\n",
    "    review = re.sub('[^a-zA-Z]',' ',X)\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    review=[review]\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review=\"My family and I are huge fans of this place. The staff is super nice, and the food is great. The chicken is very good, and the garlic sauce is perfect. Ice cream topped with fruit is delicious too. Highly recommended!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review2=\"Well presented in green and compostable  take out containers. the food had texture, it was hot, it was a nice portion but was lacking in the most relevant, flavor. The soy based sauce didn't do it for either even when I doctored it with hot sauce.I was hoping for a Thai flavor experience with a hint of lemongrass and galanga. Nope, boiled chicken served with rice and a nice hot cup of chicken broth. The broth was the best part.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[0.52 0.48]]\n",
      "[1]\n",
      "[[0.05722872 0.94277128]]\n",
      "[1]\n",
      "[[0.31167623 0.68832377]]\n",
      "[0]\n",
      "[[0.8 0.2]]\n"
     ]
    }
   ],
   "source": [
    "review_test = preparing(fake_review)\n",
    "review_test = cv.transform(review_test).toarray()\n",
    "print(load_rf.predict(review_test))\n",
    "print(load_rf.predict_proba(review_test))\n",
    "print(load_svm.predict(review_test))\n",
    "print(load_svm.predict_proba(review_test))\n",
    "print(load_lg.predict(review_test))\n",
    "print(load_lg.predict_proba(review_test))\n",
    "print(load_knn.predict(review_test))\n",
    "print(load_knn.predict_proba(review_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
